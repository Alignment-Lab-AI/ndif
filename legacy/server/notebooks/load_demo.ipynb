{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Loading large models on multi GPU with accelerate (up to 30b on 2 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from baukit import nethook\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "weights_dir = \"/share/projects/engine/weights/\"\n",
    "\n",
    "# all weights must be sharded in .bin\n",
    "MODEL_PATH = os.path.join(weights_dir, \"llama-hf/llama-30b\")\n",
    "\n",
    "\n",
    "## for weights > 30b\n",
    "if (int(re.match(\".+(\\d+)(b|B)\", MODEL_PATH).group(1)) > 30):\n",
    "    assert(torch.cuda.device_count() > 2), torch.cuda.device_count()\n",
    "else:\n",
    "    assert(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 51041271808 used for dev 0, reserved 0\n",
      "0 / 51041271808 used for dev 1, reserved 0\n"
     ]
    }
   ],
   "source": [
    "def check_dev(n):\n",
    "    t = torch.cuda.get_device_properties(n).total_memory\n",
    "    r = torch.cuda.memory_reserved(n)\n",
    "    a = torch.cuda.memory_allocated(n)\n",
    "    f = r-a  # free\n",
    "    print(f\"{a} / {t} used for dev {n}, reserved {r}\")\n",
    "    \n",
    "def check_devs():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        check_dev(i)\n",
    "\n",
    "check_devs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_SPLIT_CLASSES = [\"LlamaDecoderLayer\"]\n",
    "torch_dtype=torch.float16\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# empty init from config\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config, torch_dtype=torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 6656, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-59): 60 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "          (k_proj): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "          (v_proj): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "          (o_proj): Linear(in_features=6656, out_features=6656, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=6656, out_features=17920, bias=False)\n",
       "          (down_proj): Linear(in_features=17920, out_features=6656, bias=False)\n",
       "          (up_proj): Linear(in_features=6656, out_features=17920, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=6656, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # used to get no split blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 0,\n",
       " 'model.layers.4': 0,\n",
       " 'model.layers.5': 0,\n",
       " 'model.layers.6': 0,\n",
       " 'model.layers.7': 0,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 0,\n",
       " 'model.layers.10': 0,\n",
       " 'model.layers.11': 0,\n",
       " 'model.layers.12': 0,\n",
       " 'model.layers.13': 0,\n",
       " 'model.layers.14': 0,\n",
       " 'model.layers.15': 0,\n",
       " 'model.layers.16': 0,\n",
       " 'model.layers.17': 0,\n",
       " 'model.layers.18': 0,\n",
       " 'model.layers.19': 0,\n",
       " 'model.layers.20': 0,\n",
       " 'model.layers.21': 0,\n",
       " 'model.layers.22': 0,\n",
       " 'model.layers.23': 0,\n",
       " 'model.layers.24': 0,\n",
       " 'model.layers.25': 0,\n",
       " 'model.layers.26': 0,\n",
       " 'model.layers.27': 0,\n",
       " 'model.layers.28': 0,\n",
       " 'model.layers.29': 0,\n",
       " 'model.layers.30': 1,\n",
       " 'model.layers.31': 1,\n",
       " 'model.layers.32': 1,\n",
       " 'model.layers.33': 1,\n",
       " 'model.layers.34': 1,\n",
       " 'model.layers.35': 1,\n",
       " 'model.layers.36': 1,\n",
       " 'model.layers.37': 1,\n",
       " 'model.layers.38': 1,\n",
       " 'model.layers.39': 1,\n",
       " 'model.layers.40': 1,\n",
       " 'model.layers.41': 1,\n",
       " 'model.layers.42': 1,\n",
       " 'model.layers.43': 1,\n",
       " 'model.layers.44': 1,\n",
       " 'model.layers.45': 1,\n",
       " 'model.layers.46': 1,\n",
       " 'model.layers.47': 1,\n",
       " 'model.layers.48': 1,\n",
       " 'model.layers.49': 1,\n",
       " 'model.layers.50': 1,\n",
       " 'model.layers.51': 1,\n",
       " 'model.layers.52': 1,\n",
       " 'model.layers.53': 1,\n",
       " 'model.layers.54': 1,\n",
       " 'model.layers.55': 1,\n",
       " 'model.layers.56': 1,\n",
       " 'model.layers.57': 1,\n",
       " 'model.layers.58': 1,\n",
       " 'model.layers.59': 1,\n",
       " 'model.norm': 1,\n",
       " 'lm_head': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading, must tie weights first\n",
    "model.tie_weights()\n",
    "model = load_checkpoint_and_dispatch(\n",
    "   model, MODEL_PATH, device_map='auto', no_split_module_classes = NO_SPLIT_CLASSES \n",
    ")\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32639052800 / 51041271808 used for dev 0, reserved 32830914560\n",
      "32639066112 / 51041271808 used for dev 1, reserved 32830914560\n"
     ]
    }
   ],
   "source": [
    "check_devs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "/share/u/franlucc/transformers/src/transformers/generation/utils.py:1287: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation time: 48557750483ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The inside of a kiwi is the color of a sunset.\\nThe inside of a'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick inference test\n",
    "# Note: accelerate won't appreciate model.cuda().eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(\"The inside of a kiwi is the color\", return_tensors=\"pt\")\n",
    "inputs = inputs.to(0)\n",
    "\n",
    "time.process_time_ns()\n",
    "output = model.generate(inputs[\"input_ids\"])\n",
    "print(f\"generation time: {time.process_time_ns()}ns\")\n",
    "\n",
    "tokenizer.decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this doesn't work, need restart kernel to empty mem\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# check_devs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    '''\n",
    "    TODO: need to check for non-accelerate models\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 MODEL_NAME,\n",
    "                 MODEL_PATH,\n",
    "                 NO_SPLIT_CLASSES,\n",
    "                 torch_dtype=torch.float16) -> None:\n",
    "        \n",
    "        self.MODEL_NAME = MODEL_NAME\n",
    "        \n",
    "        # empty init\n",
    "        config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch_dtype)\n",
    "            \n",
    "        # load weights\n",
    "        # must tie weights before loading\n",
    "        model.tie_weights()\n",
    "        \n",
    "        self.model = load_checkpoint_and_dispatch(\n",
    "            model, MODEL_PATH, device_map='auto', \n",
    "            no_split_module_classes = NO_SPLIT_CLASSES \n",
    "        )\n",
    "       \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "        # check load\n",
    "        print(f\"hf_device_map ==> \\n{self.model.hf_device_map}\")\n",
    "\n",
    "        nethook.set_requires_grad(False, self.model)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        print(f\"{self.MODEL_NAME} ==> devices: {set(self.model.hf_device_map.values())}, memory: {self.model.get_memory_footprint()}\" )\n",
    "\n",
    "        self.layer_names = [\n",
    "            n\n",
    "            for n, m in self.model.named_modules()\n",
    "            if (re.match(r\"\\w+\\.(h|layers)\\.\\d+$\", n))\n",
    "        ]\n",
    "        self.num_layers = len(self.layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_device_map ==> \n",
      "{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}\n",
      "llama_7b ==> devices: {0, 1}, memory: 13543948288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "model = ModelLoader(\"llama_7b\", MODEL_PATH, [\"LlamaDecoderLayer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The inside of a kiwi is the color of a pearl.\\nThe kiwi'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick inference\n",
    "\n",
    "inputs = model.tokenizer(\"The inside of a kiwi is the color\", return_tensors=\"pt\")\n",
    "inputs = inputs.to(0)\n",
    "\n",
    "output = model.model.generate(inputs[\"input_ids\"])\n",
    "\n",
    "model.tokenizer.decode(output[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "694a090230d049b4495e42f80e317572e68bfd6d9742b8a84f760aebfe7cfa8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
